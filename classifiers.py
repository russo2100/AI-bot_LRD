# src/models/classifiers.py
from __future__ import annotations

import numpy as np
import pandas as pd
from typing import Tuple, Dict, Any, List, Optional
import logging
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class MarketDirectionClassifier:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ä—ã–Ω–∫–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥—ã –∏–∑ –∫–Ω–∏–≥–∏ "–ò–ò –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ"
    """

    def __init__(self, model_type: str = 'random_forest'):
        """
        Args:
            model_type: –¢–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ('random_forest', 'gradient_boost', 'logistic', 'svm')
        """
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.feature_columns = []
        self.target_mapping = {-1: 'DOWN', 0: 'FLAT', 1: 'UP'}

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self._init_model()

    def _init_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞"""
        if self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
        elif self.model_type == 'gradient_boost':
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        elif self.model_type == 'logistic':
            self.model = LogisticRegression(
                random_state=42,
                max_iter=1000
            )
        elif self.model_type == 'svm':
            self.model = SVC(
                kernel='rbf',
                probability=True,  # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                random_state=42
            )
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")

    def _create_target_variable(self, df: pd.DataFrame, target_column: str = 'close', 
                               threshold: float = 0.005, horizon: int = 1) -> pd.Series:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è

        Args:
            df: –î–∞–Ω–Ω—ã–µ
            target_column: –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
            horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –≤–ø–µ—Ä–µ–¥)

        Returns:
            –°–µ—Ä–∏—è —Å –º–µ—Ç–∫–∞–º–∏: -1 (–ø–∞–¥–µ–Ω–∏–µ), 0 (–±–æ–∫–æ–≤–∏–∫), 1 (—Ä–æ—Å—Ç)
        """
        if target_column not in df.columns:
            raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ {target_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ horizon –ø–µ—Ä–∏–æ–¥–æ–≤
        future_price = df[target_column].shift(-horizon)
        current_price = df[target_column]

        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        price_change = (future_price - current_price) / current_price

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        target = pd.Series(0, index=df.index)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é FLAT
        target[price_change > threshold] = 1   # UP
        target[price_change < -threshold] = -1 # DOWN

        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –Ω–µ—Ç –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        target = target[:-horizon] if horizon > 0 else target

        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: UP={sum(target==1)}, FLAT={sum(target==0)}, DOWN={sum(target==-1)}")

        return target

    def fit(self, df: pd.DataFrame, 
            feature_columns: List[str] = None,
            target_column: str = 'close',
            threshold: float = 0.005,
            horizon: int = 1,
            optimize_params: bool = False) -> 'MarketDirectionClassifier':
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞

        Args:
            df: –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            feature_columns: –ö–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            target_column: –¶–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞
            threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞
            optimize_params: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        try:
            logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ {self.model_type} –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ {len(df)} –∑–∞–ø–∏—Å—è—Ö")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if feature_columns is None:
                numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
                # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
                feature_columns = [col for col in numeric_columns if col != target_column]

            self.feature_columns = feature_columns

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            y = self._create_target_variable(df, target_column, threshold, horizon)

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—É–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å target)
            X = df[feature_columns].iloc[:len(y)]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            if len(X) < 50:
                raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)} < 50")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
            class_counts = pd.Series(y).value_counts()
            logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {dict(class_counts)}")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            X = X.fillna(X.mean())

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_scaled = self.scaler.fit_transform(X)

            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if optimize_params:
                self._optimize_hyperparameters(X_scaled, y)

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.model.fit(X_scaled, y)

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            train_score = self.model.score(X_scaled, y)

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(self.model, X_scaled, y, cv=5, scoring='accuracy')

            self.is_fitted = True

            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: Train Acc={train_score:.3f}, CV Acc={cv_scores.mean():.3f}¬±{cv_scores.std():.3f}")

            return self

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
            raise

    def _optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")

        if self.model_type == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 20],
                'min_samples_split': [2, 5, 10]
            }
        elif self.model_type == 'gradient_boost':
            param_grid = {
                'n_estimators': [50, 100, 150],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.2]
            }
        elif self.model_type == 'logistic':
            param_grid = {
                'C': [0.01, 0.1, 1.0, 10.0],
                'penalty': ['l1', 'l2']
            }
        elif self.model_type == 'svm':
            param_grid = {
                'C': [0.1, 1.0, 10.0],
                'gamma': ['scale', 'auto', 0.01, 0.1]
            }
        else:
            return

        try:
            grid_search = GridSearchCV(
                self.model, param_grid, 
                cv=3, scoring='accuracy', 
                n_jobs=-1, verbose=0
            )
            grid_search.fit(X, y)

            self.model = grid_search.best_estimator_
            logger.info(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")

    def predict(self, df: pd.DataFrame, return_probabilities: bool = False) -> np.ndarray:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è

        Args:
            df: –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            return_probabilities: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤

        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        """
        if not self.is_fitted:
            raise ValueError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ fit() –ø–µ—Ä–µ–¥ predict()")

        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X = df[self.feature_columns].fillna(df[self.feature_columns].mean())
            X_scaled = self.scaler.transform(X)

            if return_probabilities:
                probabilities = self.model.predict_proba(X_scaled)
                return probabilities
            else:
                predictions = self.model.predict(X_scaled)
                return predictions

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            raise

    def predict_single(self, features: Dict[str, float]) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
        """
        if not self.is_fitted:
            raise ValueError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            feature_vector = np.array([features.get(col, 0) for col in self.feature_columns]).reshape(1, -1)
            feature_vector_scaled = self.scaler.transform(feature_vector)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self.model.predict(feature_vector_scaled)[0]
            probabilities = self.model.predict_proba(feature_vector_scaled)[0]

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = {
                'prediction': prediction,
                'prediction_label': self.target_mapping[prediction],
                'probabilities': {
                    'DOWN': probabilities[0] if len(probabilities) > 0 else 0,
                    'FLAT': probabilities[1] if len(probabilities) > 1 else 0, 
                    'UP': probabilities[2] if len(probabilities) > 2 else 0
                },
                'confidence': max(probabilities)
            }

            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
            if hasattr(self.model, 'feature_importances_'):
                feature_importance = dict(zip(self.feature_columns, self.model.feature_importances_))
                result['feature_importance'] = feature_importance

            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            raise

    def evaluate(self, df: pd.DataFrame, target_column: str = 'close',
                threshold: float = 0.005, horizon: int = 1) -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        if not self.is_fitted:
            raise ValueError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            y_true = self._create_target_variable(df, target_column, threshold, horizon)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            X = df[self.feature_columns].iloc[:len(y_true)]
            X = X.fillna(X.mean())
            X_scaled = self.scaler.transform(X)
            y_pred = self.model.predict(X_scaled)

            # –ú–µ—Ç—Ä–∏–∫–∏
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),
                'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),
                'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),
                'samples': len(y_true)
            }

            # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
            for class_label, class_name in self.target_mapping.items():
                if class_label in y_true.values:
                    class_precision = precision_score(y_true, y_pred, labels=[class_label], average='macro', zero_division=0)
                    class_recall = recall_score(y_true, y_pred, labels=[class_label], average='macro', zero_division=0)
                    metrics[f'{class_name.lower()}_precision'] = class_precision
                    metrics[f'{class_name.lower()}_recall'] = class_recall

            logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏: Accuracy={metrics['accuracy']:.3f}, F1={metrics['f1_macro']:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")
            return {}

    def get_feature_importance(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_fitted:
            raise ValueError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        if hasattr(self.model, 'feature_importances_'):
            importance = dict(zip(self.feature_columns, self.model.feature_importances_))
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))
            return importance
        else:
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature importance")
            return {}

    def save_model(self, filepath: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.is_fitted:
            logger.error("‚ùå –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return False

        try:
            import pickle

            model_data = {
                'model_type': self.model_type,
                'model': self.model,
                'scaler': self.scaler,
                'feature_columns': self.feature_columns,
                'target_mapping': self.target_mapping,
                'is_fitted': self.is_fitted
            }

            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)

            logger.info(f"üíæ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False

    @classmethod
    def load_model(cls, filepath: str) -> 'MarketDirectionClassifier':
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            import pickle

            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)

            classifier = cls(model_type=model_data['model_type'])
            classifier.model = model_data['model']
            classifier.scaler = model_data['scaler']
            classifier.feature_columns = model_data['feature_columns']
            classifier.target_mapping = model_data['target_mapping']
            classifier.is_fitted = model_data['is_fitted']

            logger.info(f"üìÇ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω: {filepath}")
            return classifier

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            raise


class EnsembleClassifier:
    """
    –ê–Ω—Å–∞–º–±–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    """

    def __init__(self, models: List[str] = None):
        if models is None:
            models = ['random_forest', 'gradient_boost', 'logistic']

        self.models = []
        self.weights = []

        for model_type in models:
            self.models.append(MarketDirectionClassifier(model_type))

        self.is_fitted = False

    def fit(self, df: pd.DataFrame, **kwargs):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ"""
        logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ {len(self.models)} –º–æ–¥–µ–ª–µ–π")

        model_scores = []

        for i, model in enumerate(self.models):
            try:
                logger.info(f"üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {i+1}/{len(self.models)}: {model.model_type}")
                model.fit(df, **kwargs)

                # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–µ—Å–æ–≤
                metrics = model.evaluate(df, **kwargs)
                score = metrics.get('f1_macro', 0.5)
                model_scores.append(score)

                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model.model_type}: F1={score:.3f}")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {model.model_type}: {e}")
                model_scores.append(0.1)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –¥–ª—è –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏)
        total_score = sum(model_scores)
        if total_score > 0:
            self.weights = [score / total_score for score in model_scores]
        else:
            self.weights = [1.0 / len(self.models)] * len(self.models)

        self.is_fitted = True

        logger.info(f"‚úÖ –ê–Ω—Å–∞–º–±–ª—å –æ–±—É—á–µ–Ω. –í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π: {dict(zip([m.model_type for m in self.models], self.weights))}")

        return self

    def predict(self, df: pd.DataFrame, return_probabilities: bool = False):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è"""
        if not self.is_fitted:
            raise ValueError("‚ùå –ê–Ω—Å–∞–º–±–ª—å –Ω–µ –æ–±—É—á–µ–Ω")

        if return_probabilities:
            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            ensemble_probs = np.zeros((len(df), 3))  # 3 –∫–ª–∞—Å—Å–∞: DOWN, FLAT, UP

            for model, weight in zip(self.models, self.weights):
                try:
                    probs = model.predict(df, return_probabilities=True)
                    ensemble_probs += weight * probs
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model.model_type}: {e}")

            return ensemble_probs
        else:
            # –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º
            predictions = []

            for model, weight in zip(self.models, self.weights):
                try:
                    pred = model.predict(df)
                    predictions.append((pred, weight))
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model.model_type}: {e}")

            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
            if predictions:
                weighted_votes = np.zeros((len(df), 3))  # –î–ª—è 3 –∫–ª–∞—Å—Å–æ–≤

                for pred, weight in predictions:
                    for i, class_pred in enumerate(pred):
                        class_idx = {-1: 0, 0: 1, 1: 2}[class_pred]
                        weighted_votes[i, class_idx] += weight

                # –í—ã–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –≤–µ—Å–æ–º
                final_predictions = []
                for votes in weighted_votes:
                    class_idx = np.argmax(votes)
                    class_label = {0: -1, 1: 0, 2: 1}[class_idx]
                    final_predictions.append(class_label)

                return np.array(final_predictions)
            else:
                return np.zeros(len(df))


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä—ã–Ω–∫–∞")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=300, freq='D')

    # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ü–µ–Ω—ã —Å —Ç—Ä–µ–Ω–¥–æ–º
    base_price = 100
    trend = np.linspace(0, 20, 300)
    noise = np.random.randn(300) * 2
    prices = base_price + trend + noise.cumsum() * 0.5

    test_data = pd.DataFrame({
        'close': prices,
        'volume': np.random.randint(1000, 10000, 300),
        'sma_10': pd.Series(prices).rolling(10).mean().fillna(prices),
        'sma_20': pd.Series(prices).rolling(20).mean().fillna(prices),
        'rsi': np.random.uniform(20, 80, 300),
        'macd': np.random.randn(300) * 0.5,
        'volatility': pd.Series(prices).pct_change().rolling(20).std().fillna(0.02) * 100
    }, index=dates)

    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(test_data)} –∑–∞–ø–∏—Å–µ–π, {len(test_data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    try:
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Random Forest
        rf_classifier = MarketDirectionClassifier('random_forest')

        print("üéØ –û–±—É—á–µ–Ω–∏–µ Random Forest –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        rf_classifier.fit(
            test_data,
            feature_columns=['volume', 'sma_10', 'sma_20', 'rsi', 'macd', 'volatility'],
            threshold=0.01,
            horizon=1
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = rf_classifier.predict(test_data.tail(10))
        print(f"üìà –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –¥–Ω–µ–π: {predictions}")

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏
        single_pred = rf_classifier.predict_single({
            'volume': 5000,
            'sma_10': 120,
            'sma_20': 118,
            'rsi': 65,
            'macd': 0.5,
            'volatility': 2.0
        })
        print(f"üéØ –ï–¥–∏–Ω–∏—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {single_pred['prediction_label']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {single_pred['confidence']:.2f})")

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        metrics = rf_classifier.evaluate(test_data)
        print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: Accuracy={metrics.get('accuracy', 0):.3f}, F1={metrics.get('f1_macro', 0):.3f}")

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        importance = rf_classifier.get_feature_importance()
        print(f"üìã –¢–æ–ø-3 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞: {list(importance.items())[:3]}")

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        print("\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        ensemble = EnsembleClassifier(['random_forest', 'gradient_boost'])
        ensemble.fit(
            test_data,
            feature_columns=['volume', 'sma_10', 'sma_20', 'rsi', 'macd', 'volatility'],
            threshold=0.01,
            horizon=1
        )

        ensemble_predictions = ensemble.predict(test_data.tail(10))
        print(f"üèÜ –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {ensemble_predictions}")

        print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
