# src/models/lstm.py
from __future__ import annotations

import numpy as np
import pandas as pd
from typing import Tuple, Optional, Dict, Any, List
import logging
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è TensorFlow
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.regularizers import l1_l2
    TF_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tensorflow")
    TF_AVAILABLE = False

logger = logging.getLogger(__name__)

class LSTMModel:
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    –†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∫–Ω–∏–≥–∏ "–ò–ò –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ"
    """

    def __init__(self, 
                 window_size: int = 30,
                 units: int = 64,
                 dropout: float = 0.2,
                 layers: int = 2,
                 learning_rate: float = 0.001):
        """
        Args:
            window_size: –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 30 –¥–Ω–µ–π –∫–∞–∫ –≤ –∫–Ω–∏–≥–µ)
            units: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ LSTM –Ω–µ–π—Ä–æ–Ω–æ–≤
            dropout: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ LSTM —Å–ª–æ–µ–≤
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        """
        if not TF_AVAILABLE:
            raise ImportError("TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        self.window_size = window_size
        self.units = units
        self.dropout = dropout
        self.layers = layers
        self.learning_rate = learning_rate

        self.model = None
        self.scaler = None
        self.is_fitted = False
        self.feature_columns = []
        self.history = {}

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFlow –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        tf.random.set_seed(42)

    def _prepare_sequences(self, data: np.ndarray, target: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM —Å–æ–≥–ª–∞—Å–Ω–æ –º–µ—Ç–æ–¥–∏–∫–µ –∏–∑ –∫–Ω–∏–≥–∏
        """
        if len(data) < self.window_size + 1:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(data)} < {self.window_size + 1}")

        X, y = [], []

        for i in range(len(data) - self.window_size):
            # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            X.append(data[i:i + self.window_size])

            # –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (—Å–ª–µ–¥—É—é—â–∏–π –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
            if target is not None:
                y.append(target[i + self.window_size])
            else:
                # –ï—Å–ª–∏ target –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü data
                y.append(data[i + self.window_size, -1])

        X = np.array(X)
        y = np.array(y)

        logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {X.shape[0]}, —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {X.shape[1]}, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[2]}")

        return X, y

    def _build_model(self, input_shape: Tuple[int, int]) -> Sequential:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LSTM –º–æ–¥–µ–ª–∏
        """
        model = Sequential()

        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        model.add(Input(shape=input_shape))

        # LSTM —Å–ª–æ–∏
        for i in range(self.layers):
            return_sequences = (i < self.layers - 1)  # –í—Å–µ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è

            model.add(LSTM(
                units=self.units,
                return_sequences=return_sequences,
                dropout=self.dropout,
                recurrent_dropout=self.dropout,
                kernel_regularizer=l1_l2(l1=0.01, l2=0.01)
            ))

            # Batch normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
            if i < self.layers - 1:
                model.add(BatchNormalization())

        # –í—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
        model.add(Dropout(self.dropout))
        model.add(Dense(32, activation='relu'))
        model.add(Dropout(self.dropout / 2))
        model.add(Dense(1))  # –û–¥–∏–Ω –≤—ã—Ö–æ–¥ –¥–ª—è —Ü–µ–Ω—ã

        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
        optimizer = Adam(learning_rate=self.learning_rate)
        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

        return model

    def fit(self, 
            df: pd.DataFrame,
            target_column: str = 'close',
            feature_columns: List[str] = None,
            validation_split: float = 0.2,
            epochs: int = 100,
            batch_size: int = 32,
            verbose: int = 1) -> 'LSTMModel':
        """
        –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏

        Args:
            df: –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            target_column: –¶–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            feature_columns: –ö–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (None = –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ)
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            verbose: –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–∞
        """
        try:
            logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ –Ω–∞ {len(df)} –∑–∞–ø–∏—Å—è—Ö")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if feature_columns is None:
                numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
                # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –∫–æ–ª–æ–Ω–∫—É –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –µ—Å–ª–∏ –æ–Ω–∞ —Ç–∞–º –µ—Å—Ç—å
                feature_columns = [col for col in numeric_columns if col != target_column]

            self.feature_columns = feature_columns

            if target_column not in df.columns:
                raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ '{target_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            from sklearn.preprocessing import MinMaxScaler
            self.scaler = MinMaxScaler()

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            features_data = df[feature_columns].values
            target_data = df[target_column].values

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            features_scaled = self.scaler.fit_transform(features_data)
            target_scaled = (target_data - target_data.min()) / (target_data.max() - target_data.min())

            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            X, y = self._prepare_sequences(features_scaled, target_scaled)

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
            split_idx = int(len(X) * (1 - validation_split))
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]

            logger.info(f"üìä Train: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, Val: {X_val.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            input_shape = (X_train.shape[1], X_train.shape[2])
            self.model = self._build_model(input_shape)

            logger.info(f"üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM: {self.layers} —Å–ª–æ–µ–≤ –ø–æ {self.units} –Ω–µ–π—Ä–æ–Ω–æ–≤")

            # Callbacks –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=15,
                    restore_best_weights=True,
                    verbose=verbose
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.7,
                    patience=8,
                    min_lr=1e-6,
                    verbose=verbose
                )
            ]

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=verbose,
                shuffle=True
            )

            self.history = {
                'loss': history.history['loss'],
                'val_loss': history.history['val_loss'],
                'mae': history.history['mae'],
                'val_mae': history.history['val_mae']
            }

            self.is_fitted = True

            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            final_loss = history.history['val_loss'][-1]
            final_mae = history.history['val_mae'][-1]

            logger.info(f"‚úÖ LSTM –æ–±—É—á–µ–Ω–∞: Val Loss={final_loss:.6f}, Val MAE={final_mae:.6f}")

            return self

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è LSTM: {e}")
            raise

    def predict(self, 
                df: pd.DataFrame, 
                steps: int = 1,
                return_sequences: bool = False) -> np.ndarray:
        """
        –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π LSTM –º–æ–¥–µ–ª–∏

        Args:
            df: –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∞
            return_sequences: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã
        """
        if not self.is_fitted:
            raise ValueError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ fit() –ø–µ—Ä–µ–¥ predict()")

        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            features_data = df[self.feature_columns].tail(self.window_size).values
            features_scaled = self.scaler.transform(features_data)

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è LSTM
            X = features_scaled.reshape(1, self.window_size, len(self.feature_columns))

            predictions = []
            current_sequence = X.copy()

            # –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            for step in range(steps):
                # –ü—Ä–æ–≥–Ω–æ–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                pred = self.model.predict(current_sequence, verbose=0)
                predictions.append(pred[0, 0])

                if step < steps - 1:
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
                    # –°–¥–≤–∏–≥–∞–µ–º –æ–∫–Ω–æ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç
                    new_sequence = np.roll(current_sequence, -1, axis=1)
                    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –∫–∞–∫ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
                    new_sequence[0, -1, :] = pred[0, 0]  
                    current_sequence = new_sequence

            predictions = np.array(predictions)

            if return_sequences:
                logger.info(f"üìà –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ {steps} –ø–µ—Ä–∏–æ–¥–æ–≤")
                return predictions
            else:
                logger.info(f"üìà –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø–µ—Ä–∏–æ–¥: {predictions[0]:.6f}")
                return predictions[0]

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            raise

    def evaluate(self, df: pd.DataFrame, target_column: str = 'close') -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        if not self.is_fitted:
            raise ValueError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            features_data = df[self.feature_columns].values
            target_data = df[target_column].values

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler)
            features_scaled = self.scaler.transform(features_data)
            target_scaled = (target_data - target_data.min()) / (target_data.max() - target_data.min())

            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            X, y = self._prepare_sequences(features_scaled, target_scaled)

            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            loss, mae = self.model.evaluate(X, y, verbose=0)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            predictions = self.model.predict(X, verbose=0).flatten()

            # RMSE
            rmse = np.sqrt(np.mean((y - predictions) ** 2))

            # MAPE (Mean Absolute Percentage Error)
            mape = np.mean(np.abs((y - predictions) / (y + 1e-10))) * 100

            # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π)
            actual_direction = np.sign(np.diff(y))
            predicted_direction = np.sign(np.diff(predictions))
            directional_accuracy = np.mean(actual_direction == predicted_direction) * 100

            metrics = {
                'loss': loss,
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'directional_accuracy': directional_accuracy,
                'samples': len(y)
            }

            logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: MAE={mae:.6f}, RMSE={rmse:.6f}, DA={directional_accuracy:.1f}%")

            return metrics

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")
            return {}

    def plot_training_history(self) -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.history:
            logger.warning("‚ö†Ô∏è –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return

        try:
            import matplotlib.pyplot as plt

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

            # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
            ax1.plot(self.history['loss'], label='Train Loss')
            ax1.plot(self.history['val_loss'], label='Val Loss')
            ax1.set_title('Model Loss')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.legend()
            ax1.grid(True)

            # –ì—Ä–∞—Ñ–∏–∫ MAE
            ax2.plot(self.history['mae'], label='Train MAE')
            ax2.plot(self.history['val_mae'], label='Val MAE')
            ax2.set_title('Model MAE')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('MAE')
            ax2.legend()
            ax2.grid(True)

            plt.tight_layout()
            plt.show()

        except ImportError:
            logger.warning("‚ö†Ô∏è Matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}")

    def save_model(self, filepath: str) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        if not self.is_fitted:
            logger.error("‚ùå –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return False

        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å TensorFlow
            model_path = filepath.replace('.pkl', '_model.h5')
            self.model.save(model_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            import pickle
            additional_data = {
                'window_size': self.window_size,
                'units': self.units,
                'dropout': self.dropout,
                'layers': self.layers,
                'learning_rate': self.learning_rate,
                'feature_columns': self.feature_columns,
                'scaler': self.scaler,
                'history': self.history,
                'is_fitted': self.is_fitted
            }

            with open(filepath, 'wb') as f:
                pickle.dump(additional_data, f)

            logger.info(f"üíæ LSTM –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}, {model_path}")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False

    @classmethod
    def load_model(cls, filepath: str) -> 'LSTMModel':
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        try:
            import pickle

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with open(filepath, 'rb') as f:
                data = pickle.load(f)

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏
            model = cls(
                window_size=data['window_size'],
                units=data['units'],
                dropout=data['dropout'],
                layers=data['layers'],
                learning_rate=data['learning_rate']
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º TensorFlow –º–æ–¥–µ–ª—å
            model_path = filepath.replace('.pkl', '_model.h5')
            model.model = tf.keras.models.load_model(model_path)

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            model.feature_columns = data['feature_columns']
            model.scaler = data['scaler']
            model.history = data['history']
            model.is_fitted = data['is_fitted']

            logger.info(f"üìÇ LSTM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
            return model

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            raise


class GRUModel(LSTMModel):
    """
    GRU –º–æ–¥–µ–ª—å - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ LSTM —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """

    def _build_model(self, input_shape: Tuple[int, int]) -> Sequential:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã GRU –º–æ–¥–µ–ª–∏
        """
        model = Sequential()

        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        model.add(Input(shape=input_shape))

        # GRU —Å–ª–æ–∏ (–±—ã—Å—Ç—Ä–µ–µ LSTM)
        for i in range(self.layers):
            return_sequences = (i < self.layers - 1)

            model.add(GRU(
                units=self.units,
                return_sequences=return_sequences,
                dropout=self.dropout,
                recurrent_dropout=self.dropout,
                kernel_regularizer=l1_l2(l1=0.01, l2=0.01)
            ))

            if i < self.layers - 1:
                model.add(BatchNormalization())

        # –í—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
        model.add(Dropout(self.dropout))
        model.add(Dense(32, activation='relu'))
        model.add(Dropout(self.dropout / 2))
        model.add(Dense(1))

        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
        optimizer = Adam(learning_rate=self.learning_rate)
        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

        return model


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏")

    if not TF_AVAILABLE:
        print("‚ùå TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        exit(1)

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=200, freq='D')

    # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ü–µ–Ω—ã —Å —Ç—Ä–µ–Ω–¥–æ–º –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
    trend = np.linspace(100, 120, 200)
    noise = np.random.randn(200) * 2

    test_data = pd.DataFrame({
        'close': trend + noise,
        'volume': np.random.randint(1000, 10000, 200),
        'sma_10': (trend + noise) * 0.98,  # –∏–º–∏—Ç–∞—Ü–∏—è SMA
        'rsi': np.random.uniform(30, 70, 200),  # –∏–º–∏—Ç–∞—Ü–∏—è RSI
        'macd': np.random.randn(200) * 0.5
    }, index=dates)

    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(test_data)} –∑–∞–ø–∏—Å–µ–π, {len(test_data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ LSTM
        lstm_model = LSTMModel(window_size=20, units=32, layers=1)

        print("üéØ –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏...")
        lstm_model.fit(
            test_data,
            target_column='close',
            feature_columns=['close', 'volume', 'sma_10', 'rsi', 'macd'],
            epochs=20,  # –ú–∞–ª–æ —ç–ø–æ—Ö –¥–ª—è —Ç–µ—Å—Ç–∞
            batch_size=16,
            verbose=0
        )

        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
        prediction = lstm_model.predict(test_data.tail(30))
        print(f"üìà –ü—Ä–æ–≥–Ω–æ–∑: {prediction:.4f}")

        # –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑
        multi_pred = lstm_model.predict(test_data.tail(30), steps=5, return_sequences=True)
        print(f"üìä –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 5 —à–∞–≥–æ–≤: {multi_pred}")

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        metrics = lstm_model.evaluate(test_data.tail(50))
        print(f"üìä MAE: {metrics.get('mae', 0):.6f}")
        print(f"üìä –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {metrics.get('directional_accuracy', 0):.1f}%")

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GRU
        print("\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GRU –º–æ–¥–µ–ª–∏...")
        gru_model = GRUModel(window_size=20, units=32, layers=1)

        gru_model.fit(
            test_data,
            target_column='close',
            feature_columns=['close', 'volume', 'sma_10', 'rsi', 'macd'],
            epochs=20,
            batch_size=16,
            verbose=0
        )

        gru_prediction = gru_model.predict(test_data.tail(30))
        print(f"üìà GRU –ø—Ä–æ–≥–Ω–æ–∑: {gru_prediction:.4f}")

        print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LSTM/GRU –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
