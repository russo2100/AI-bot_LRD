# src/data/preprocess.py
"""
Data preprocessing module for financial time series analysis.

Supported CSV format:
Required columns:
- date: DateTime column in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS
- open: Opening price (float)
- high: Highest price (float)
- low: Lowest price (float)
- close: Closing price (float)
- volume: Trading volume (integer/float, optional)

Optional columns:
- adj_close: Adjusted closing price
- symbol: Trading symbol/id
- dividends: Dividend payments
- stock_splits: Stock split ratios
- timestamp: Unix timestamp

Example CSV:
date,open,high,low,close,volume
2023-01-01,150.00,155.00,149.50,154.00,1000000
2023-01-02,154.10,158.00,153.00,157.50,1100000
"""

from __future__ import annotations

import numpy as np
import pandas as pd
from typing import Tuple, Optional, Dict, Any, List
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import logging
import warnings
from datetime import datetime

logger = logging.getLogger(__name__)

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º –∫–Ω–∏–≥–∏:
    - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    - –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    """
    if df.empty:
        return df

    logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –∏—Å—Ö–æ–¥–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")

    df_clean = df.copy()

    # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å—É (–≤—Ä–µ–º–µ–Ω–∏)
    before_dupes = len(df_clean)
    df_clean = df_clean[~df_clean.index.duplicated(keep='last')]
    if len(df_clean) < before_dupes:
        logger.info(f"üîÑ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dupes - len(df_clean)}")

    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ OHLCV –¥–∞–Ω–Ω—ã—Ö
    numeric_cols = ['open', 'high', 'low', 'close', 'volume']
    existing_cols = [col for col in numeric_cols if col in df_clean.columns]

    for col in existing_cols:
        # –£–±—Ä–∞—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω
        if col != 'volume':
            negative_count = (df_clean[col] <= 0).sum()
            if negative_count > 0:
                logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {negative_count} –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ {col}")
                df_clean[col] = df_clean[col].clip(lower=0.01)

        # –£–±—Ä–∞—Ç—å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã (–±–æ–ª—å—à–µ 10 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
        mean_val = df_clean[col].mean()
        std_val = df_clean[col].std()
        outlier_threshold = 10

        outliers = abs(df_clean[col] - mean_val) > outlier_threshold * std_val
        outlier_count = outliers.sum()

        if outlier_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {outlier_count} –≤—ã–±—Ä–æ—Å–æ–≤ –≤ {col}")
            # –ó–∞–º–µ–Ω—è–µ–º –≤—ã–±—Ä–æ—Å—ã –º–µ–¥–∏–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
            df_clean.loc[outliers, col] = df_clean[col].median()

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ OHLC –¥–∞–Ω–Ω—ã—Ö
    if all(col in df_clean.columns for col in ['open', 'high', 'low', 'close']):
        # High –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å >= max(open, close)
        invalid_high = df_clean['high'] < df_clean[['open', 'close']].max(axis=1)
        # Low –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å <= min(open, close)  
        invalid_low = df_clean['low'] > df_clean[['open', 'close']].min(axis=1)

        invalid_count = (invalid_high | invalid_low).sum()
        if invalid_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {invalid_count} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö OHLC –∑–∞–ø–∏—Å–µ–π")
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            df_clean.loc[invalid_high, 'high'] = df_clean.loc[invalid_high, ['open', 'close']].max(axis=1)
            df_clean.loc[invalid_low, 'low'] = df_clean.loc[invalid_low, ['open', 'close']].min(axis=1)

    # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    missing_count = df_clean.isnull().sum().sum()
    if missing_count > 0:
        logger.info(f"üîß –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ {missing_count} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

        # –î–ª—è —Ü–µ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill, –∑–∞—Ç–µ–º backward fill
        price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in df_clean.columns]
        df_clean[price_cols] = df_clean[price_cols].fillna(method='ffill').fillna(method='bfill')

        # –î–ª—è –æ–±—ä–µ–º–∞ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        if 'volume' in df_clean.columns:
            df_clean['volume'] = df_clean['volume'].fillna(0)

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        df_clean = df_clean.fillna(0)

    logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(df_clean)} –∑–∞–ø–∏—Å–µ–π")
    return df_clean


def normalize_data(df: pd.DataFrame, method: str = 'minmax', 
                  columns: Optional[list] = None) -> Tuple[pd.DataFrame, dict]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –≤ –∫–Ω–∏–≥–µ

    Args:
        df: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        method: –ú–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ ('minmax', 'standard', 'robust')
        columns: –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (None = –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ)

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–ª–æ–≤–∞—Ä—å —Å scaler'–∞–º–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    """
    if df.empty:
        return df, {}

    df_norm = df.copy()
    scalers = {}

    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    if columns is None:
        numeric_columns = df_norm.select_dtypes(include=[np.number]).columns.tolist()
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å
        exclude_columns = ['volume', 'is_complete']
        columns = [col for col in numeric_columns if col not in exclude_columns]

    logger.info(f"üìè –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º '{method}' –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫: {columns}")

    for col in columns:
        if col not in df_norm.columns:
            logger.warning(f"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            continue

        values = df_norm[col].values.reshape(-1, 1)

        # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'standard':
            scaler = StandardScaler()
        elif method == 'robust':
            from sklearn.preprocessing import RobustScaler
            scaler = RobustScaler()
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {method}")

        # –û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        normalized_values = scaler.fit_transform(values).flatten()
        df_norm[col] = normalized_values
        scalers[col] = scaler

    logger.info(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è {len(columns)} –∫–æ–ª–æ–Ω–æ–∫")
    return df_norm, scalers


def denormalize_data(df_normalized: pd.DataFrame, scalers: dict) -> pd.DataFrame:
    """–û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    df_denorm = df_normalized.copy()

    for col, scaler in scalers.items():
        if col in df_denorm.columns:
            values = df_denorm[col].values.reshape(-1, 1)
            denorm_values = scaler.inverse_transform(values).flatten()
            df_denorm[col] = denorm_values

    return df_denorm


def create_returns_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ (–æ—Å–Ω–æ–≤–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–Ω–∏–≥–µ)
    """
    if df.empty or 'close' not in df.columns:
        return df

    df_features = df.copy()

    # –ü—Ä–æ—Å—Ç—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
    df_features['returns_1d'] = df_features['close'].pct_change(1)
    df_features['returns_5d'] = df_features['close'].pct_change(5) 
    df_features['returns_20d'] = df_features['close'].pct_change(20)

    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ (–ª—É—á—à–µ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
    df_features['log_returns_1d'] = np.log(df_features['close'] / df_features['close'].shift(1))
    df_features['log_returns_5d'] = np.log(df_features['close'] / df_features['close'].shift(5))

    # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è)
    df_features['volatility_10d'] = df_features['returns_1d'].rolling(10).std()
    df_features['volatility_20d'] = df_features['returns_1d'].rolling(20).std()

    # Z-score —Ü–µ–Ω—ã (–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö —Å—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è)
    df_features['price_zscore_20d'] = (df_features['close'] - df_features['close'].rolling(20).mean()) / df_features['close'].rolling(20).std()

    # –ó–∞–ø–æ–ª–Ω–∏—Ç—å NaN
    df_features = df_features.fillna(0)

    logger.info("‚ú® –°–æ–∑–¥–∞–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏")
    return df_features


def create_time_windows(df: pd.DataFrame, window_size: int = 30, 
                       target_col: str = 'close', step: int = 1) -> Tuple[np.ndarray, np.ndarray]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM/GRU —Å–æ–≥–ª–∞—Å–Ω–æ –∫–Ω–∏–≥–µ

    Args:
        df: –î–∞–Ω–Ω—ã–µ
        window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30 –∫–∞–∫ –≤ –∫–Ω–∏–≥–µ)
        target_col: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        step: –®–∞–≥ –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏

    Returns:
        X: –í—Ö–æ–¥–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (samples, timesteps, features)
        y: –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (samples,)
    """
    if len(df) <= window_size:
        logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)} <= {window_size}")
        return np.array([]), np.array([])

    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # –£–±–∏—Ä–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –µ—Å–ª–∏ –æ–Ω–∞ —Ç–∞–º –µ—Å—Ç—å
    feature_cols = [col for col in numeric_cols if col != target_col]

    if not feature_cols:
        logger.error("‚ùå –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω")
        return np.array([]), np.array([])

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    features = df[feature_cols].values
    targets = df[target_col].values

    X, y = [], []

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–æ–Ω
    for i in range(0, len(df) - window_size, step):
        # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        X.append(features[i:i + window_size])
        # –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (—Å–ª–µ–¥—É—é—â–µ–µ –ø–æ—Å–ª–µ –æ–∫–Ω–∞)
        y.append(targets[i + window_size])

    X = np.array(X)
    y = np.array(y)

    logger.info(f"ü™ü –°–æ–∑–¥–∞–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω: {len(X)} –≤—ã–±–æ—Ä–æ–∫, —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {window_size}, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")

    return X, y


def split_data(df: pd.DataFrame, train_ratio: float = 0.8, 
              val_ratio: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/validation/test —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    """
    n = len(df)
    train_size = int(n * train_ratio)
    val_size = int(n * val_ratio)

    train_data = df.iloc[:train_size]
    val_data = df.iloc[train_size:train_size + val_size] 
    test_data = df.iloc[train_size + val_size:]

    logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")

    return train_data, val_data, test_data


def add_lagged_features(df: pd.DataFrame, columns: list, lags: list = [1, 2, 3, 5]) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∞–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤–∞–∂–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
    """
    df_lagged = df.copy()

    for col in columns:
        if col not in df.columns:
            continue

        for lag in lags:
            lag_col_name = f"{col}_lag_{lag}"
            df_lagged[lag_col_name] = df_lagged[col].shift(lag)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª—è–º–∏
    df_lagged = df_lagged.fillna(0)

    logger.info(f"‚è≥ –î–æ–±–∞–≤–ª–µ–Ω—ã –ª–∞–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è {len(columns)} –∫–æ–ª–æ–Ω–æ–∫ —Å –ª–∞–≥–∞–º–∏ {lags}")
    return df_lagged


def detect_outliers(df: pd.DataFrame, column: str, method: str = 'iqr', 
                   threshold: float = 1.5) -> pd.Series:
    """
    –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

    Args:
        df: –î–∞–Ω–Ω—ã–µ
        column: –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        method: –ú–µ—Ç–æ–¥ ('iqr', 'zscore', 'isolation_forest')
        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

    Returns:
        –ë—É–ª–µ–≤–∞ —Å–µ—Ä–∏—è —Å True –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤
    """
    if column not in df.columns:
        return pd.Series([False] * len(df), index=df.index)

    values = df[column]

    if method == 'iqr':
        Q1 = values.quantile(0.25)
        Q3 = values.quantile(0.75)
        IQR = Q3 - Q1
        outliers = (values < (Q1 - threshold * IQR)) | (values > (Q3 + threshold * IQR))

    elif method == 'zscore':
        z_scores = abs((values - values.mean()) / values.std())
        outliers = z_scores > threshold

    elif method == 'isolation_forest':
        try:
            from sklearn.ensemble import IsolationForest
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            outlier_labels = iso_forest.fit_predict(values.values.reshape(-1, 1))
            outliers = pd.Series(outlier_labels == -1, index=df.index)
        except ImportError:
            logger.warning("‚ö†Ô∏è Sklearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ IQR")
            return detect_outliers(df, column, method='iqr', threshold=threshold)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")

    outlier_count = outliers.sum()
    logger.info(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ {column} –º–µ—Ç–æ–¥–æ–º {method}: {outlier_count} ({outlier_count/len(df)*100:.1f}%)")

    return outliers


def validate_csv_format(file_path: str) -> Tuple[bool, List[str]]:
    """
    Validate CSV file format for financial data

    Args:
        file_path: Path to CSV file

    Returns:
        Tuple of (is_valid, error_messages)
    """
    try:
        df = pd.read_csv(file_path, nrows=5)  # Read only first 5 rows for validation
    except Exception as e:
        return False, [f"Cannot read CSV file: {e}"]

    errors = []

    # Check required columns
    required_columns = ['date', 'open', 'high', 'low', 'close']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        errors.append(f"Missing required columns: {missing_columns}")

    # Validate date column
    if 'date' in df.columns:
        try:
            pd.to_datetime(df['date'], errors='raise')
        except Exception:
            errors.append("Invalid date format in 'date' column")

    # Validate numeric columns
    numeric_columns = ['open', 'high', 'low', 'close']
    for col in numeric_columns:
        if col in df.columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                errors.append(f"Column '{col}' must be numeric")

    # Check for positive prices
    if 'close' in df.columns:
        if (df['close'] <= 0).any():
            errors.append("Found non-positive closing prices")

    return len(errors) == 0, errors


def load_and_validate_csv(file_path: str, date_column: str = 'date',
                         parse_dates: bool = True) -> Tuple[Optional[pd.DataFrame], List[str]]:
    """
    Load CSV file with validation and proper date parsing

    Args:
        file_path: Path to CSV file
        date_column: Name of date column
        parse_dates: Whether to parse dates

    Returns:
        Tuple of (dataframe or None, error messages)
    """
    errors = []

    try:
        # Read CSV with date parsing
        if parse_dates and date_column:
            df = pd.read_csv(file_path, parse_dates=[date_column])
            if date_column in df.columns:
                df = df.sort_values(date_column).set_index(date_column)
        else:
            df = pd.read_csv(file_path)

        # Basic validation
        if df.empty:
            errors.append("CSV file is empty")
            return None, errors

        # Remove duplicates
        initial_len = len(df)
        df = df.drop_duplicates()
        if len(df) < initial_len:
            warnings.warn(f"Removed {initial_len - len(df)} duplicate rows")

        logger.info(f"Successfully loaded CSV: {len(df)} rows, {len(df.columns)} columns")
        return df, errors

    except Exception as e:
        errors.append(f"Error loading CSV: {e}")
        return None, errors


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    dates = pd.date_range('2023-01-01', periods=100, freq='H')
    test_data = pd.DataFrame({
        'open': np.random.randn(100).cumsum() + 100,
        'high': np.random.randn(100).cumsum() + 105,
        'low': np.random.randn(100).cumsum() + 95,
        'close': np.random.randn(100).cumsum() + 100,
        'volume': np.random.randint(1000, 10000, 100)
    }, index=dates)

    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    test_data.loc[test_data.index[50], 'close'] = -10  # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞
    test_data.loc[test_data.index[60], 'high'] = 1000   # –≤—ã–±—Ä–æ—Å

    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏
    cleaned_data = clean_data(test_data)
    print(f"‚úÖ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_data)} –∑–∞–ø–∏—Å–µ–π")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    normalized_data, scalers = normalize_data(cleaned_data)
    print(f"üìè –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –¥–ª—è {len(scalers)} –∫–æ–ª–æ–Ω–æ–∫")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω
    X, y = create_time_windows(normalized_data, window_size=10)
    print(f"ü™ü –°–æ–∑–¥–∞–Ω–æ –æ–∫–æ–Ω: {len(X)} –≤—ã–±–æ—Ä–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–º {X.shape[1]}x{X.shape[2]}")

    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")
